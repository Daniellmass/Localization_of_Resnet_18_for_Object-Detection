# -*- coding: utf-8 -*-
"""Yet another copy of birds ObjDetection part3 works for daniel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECJmiGGn92LgCF1DPsnQGaMtUFAdRJHu
"""

from google.colab import drive
drive.mount('/content/drive')

import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import json
from PIL import Image
import os
import torch
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import torchvision.transforms as T
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import json
import os
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

import albumentations
print(albumentations.__version__)

model_path = "/content/drive/MyDrive/voc_data/155penguin_multy_0319.pth"

#@title testing functions
import torch
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision.ops import nms

def run_visualize_test(
    model_path,
    root_dir,             # e.g. the folder with JPEGImages/ and Annotations/
    set_filename,         # e.g. "/content/drive/MyDrive/voc_data/ImageSets/Main/bird_val.txt"
    device='cpu'
):
    """
    Loads your multi-object detection model,
    runs it on the first 20 images of the specified dataset,
    and visualizes predictions vs. ground truth.
    """
    from torchvision import transforms as T

    # 1) Build or load your model (multi-object version).
    model = MultiBirdModel_Anchors(num_classes=2)  # or your chosen model class
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    # 2) Albumentations transforms for test
    test_transform = A.Compose([
        A.Resize(224, 224),
        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
        ToTensorV2()
    ],
    bbox_params=A.BboxParams(
        format='pascal_voc',     # <-- IMPORTANT: match [xmin,ymin,xmax,ymax]
        label_fields=['class_labels'],
        min_area=0.0,
        min_visibility=0.0
    ))

    # 3) Create dataset (include_background=False => only bird-labeled images)
    test_dataset = VOC_PenguinDB(
      root_dir="/content/drive/MyDrive/penguin_db",
      split="test",
      transform=test_transform,
      include_background=False
    )
    print("Filtered test dataset length:", len(test_dataset))

    # 4) Pick a few samples to visualize
    indices_to_show = list(range(min(20, len(test_dataset))))
    for idx in indices_to_show:
        # fetch the sample
        image_tensor, boxes_gt, labels_gt, img_id = test_dataset[idx]
        print(f"\n=== Sample {idx} / Image ID: {img_id} ===")

        # run inference
        image_batch = image_tensor.unsqueeze(0).to(device)
        with torch.no_grad():
            cls_map_out, box_map_out = model(image_batch)
        # decode to get final predicted boxes
        (pred_boxes, pred_scores) = decode_predictions(cls_map_out, box_map_out)[0]

        # visualize
        visualize_multi_prediction(
            image_tensor,
            boxes_gt,
            pred_boxes,
            title=f"ID: {img_id}, Index: {idx}"
        )

def visualize_multi_prediction(image_tensor, boxes_gt, boxes_pred, title=""):
    """
    Draws multiple ground-truth boxes in red, predicted boxes in blue.
    boxes_gt: shape [N,4] with [x,y,w,h] or [x1,y1,x2,y2] (adapt as needed)
    boxes_pred: shape [M,4] in [x1,y1,x2,y2]
    """
    from torchvision import transforms as T
    import numpy as np

    # 1) Undo the normalization for display
    inv_normalize = T.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    denorm = inv_normalize(image_tensor.clone()).clamp(0,1)
    np_image = denorm.permute(1,2,0).cpu().numpy()

    # 2) If your GT boxes are [x,y,w,h], convert them to x1,y1,x2,y2
    boxes_gt_xyxy = []
    for b in boxes_gt:
        x, y, w, h = b
        x2 = x + w
        y2 = y + h
        boxes_gt_xyxy.append([x.item(), y.item(), x2.item(), y2.item()])

    # 3) Plot
    fig, ax = plt.subplots(1, figsize=(6,6))
    ax.imshow(np_image)
    ax.set_title(title)

    # draw ground-truth boxes in red
    for b_gt in boxes_gt_xyxy:
        x1, y1, x2, y2 = b_gt
        rect_gt = patches.Rectangle(
            (x1, y1), x2 - x1, y2 - y1,
            linewidth=2, edgecolor='r', facecolor='none'
        )
        ax.add_patch(rect_gt)

    # draw predicted boxes in blue
    for b_pred in boxes_pred:
        x1, y1, x2, y2 = b_pred
        rect_pred = patches.Rectangle(
            (x1, y1), x2 - x1, y2 - y1,
            linewidth=2, edgecolor='b', facecolor='none'
        )
        ax.add_patch(rect_pred)

    plt.show()

#@title debugStuff

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision import transforms as T

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import torchvision.transforms as T

def debug_ground_truth(dataset, num_samples=20):
    """
    Plots ground-truth bounding boxes from both the start and end of the dataset.

    dataset: a VOC_BirdDataset or similar that returns:
             (image_out, boxes, labels, img_id)
    num_samples: number of images to plot from the start and end.
                 (i.e., 20 will take 20 from start + 20 from end)
    """
    total_samples = len(dataset)

    # Ensure num_samples does not exceed half the dataset size
    num_samples = min(num_samples, total_samples // 2)
    indices =  list(range(num_samples)) + list(range(total_samples - num_samples, total_samples))

    for idx in indices:
        image_out, boxes, labels, img_id = dataset[idx]

        # Convert image tensor to a NumPy array for visualization
        inv_normalize = T.Normalize(
            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
            std=[1/0.229, 1/0.224, 1/0.225]
        )

        denorm = inv_normalize(image_out.clone()).clamp(0, 1)
        np_img = denorm.permute(1, 2, 0).cpu().numpy()

        fig, ax = plt.subplots(figsize=(6, 6))
        ax.imshow(np_img)
        ax.set_title(f"Image ID: {img_id}\nBoxes: {boxes.size(0)} / Labels: {labels.tolist()}")

        # Draw bounding boxes
        for b_idx in range(boxes.size(0)):
            x, y, w, h = boxes[b_idx]
            rect = patches.Rectangle(
                (x.item(), y.item()),  # (left, top)
                w.item(), h.item(),    # width, height
                linewidth=2,
                edgecolor='r',
                facecolor='none'
            )
            ax.add_patch(rect)

        plt.show()




def visualize_original_annotation(images_dir, json_file, image_id):
    """
    Loads an image and its ground truth bounding box (without any transforms)
    and plots them together.

    Parameters:
      images_dir (str): Directory where the images are stored.
      json_file (str): Path to the COCO-format JSON file.
      image_id (int): The ID of the image to visualize.
    """
    # Load the JSON annotations.
    with open(json_file, 'r') as f:
        coco = json.load(f)

    # Find the image info with the specified image_id.
    image_info = next((img for img in coco['images'] if img['id'] == image_id), None)
    if image_info is None:
        print(f"Image id {image_id} not found.")
        return

    # Construct the full image path and load the image.
    image_path = os.path.join(images_dir, image_info['file_name'])
    image = Image.open(image_path).convert("RGB")

    # Retrieve the annotations for this image.
    anns = [ann for ann in coco['annotations'] if ann['image_id'] == image_id]
    if not anns:
        print(f"No annotations found for image id {image_id}.")
        return

    # For this example, assume we are interested in the first annotation.
    ann = anns[0]
    bbox = ann['bbox']  # Typically in [x, y, width, height] format.

    # Plot the image and draw the bounding box.
    fig, ax = plt.subplots(1, figsize=(8, 8))
    ax.imshow(image)
    x, y, w, h = bbox
    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)
    ax.set_title(f"Image ID {image_id} - Ground Truth")
    plt.show()

# Example usage:
# Change the paths and image_id to one that exists in your dataset.
# visualize_original_annotation("db1/train", "db1/train/_annotations.coco.json", image_id=353)


def run_inference(image_path, model, transform):
    """
    Loads a single image, applies the transform, and runs inference on it.
    Returns the predicted bounding box and predicted class.
    """
    # Load the image using PIL
    image = Image.open(image_path).convert("RGB")

    # Save a copy of the original image for visualization.
    orig_image = image.copy()

    # Apply the same transform used during validation (without data augmentation)
    image_tensor = transform(image).unsqueeze(0)  # add batch dimension

    # Run inference (with no gradient computation)
    model.eval()  # ensure model is in eval mode
    with torch.no_grad():
        cls_logits, bbox_pred = model(image_tensor)

    # Get the predicted bounding box (assumes bbox_pred shape is [1, 4])
    bbox = bbox_pred[0].cpu().numpy()

    # Get predicted class label (0=background, 1=penguin)
    pred_class = torch.argmax(cls_logits, dim=1)[0].item()

    return orig_image, bbox, pred_class

def visualize_prediction(image, bbox, pred_class):
    """
    Displays the image with the predicted bounding box.
    """
    fig, ax = plt.subplots(1)
    ax.imshow(image)

    # Here we assume bbox is in [x, y, width, height] format.
    x, y, w, h = bbox
    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)

    label_text = "penguin" if pred_class == 1 else "background"
    plt.title(f"Predicted: {label_text}")
    plt.show()

def visualize_tensor(image_tensor):
    # image_tensor: shape [3, H, W], normalized
    # Build an inverse transform with the negative of the mean/std used
    inv_normalize = T.Normalize(
        mean=[-m/s for m, s in zip([0.485, 0.456, 0.406],
                                   [0.229, 0.224, 0.225])],
        std=[1/s for s in [0.229, 0.224, 0.225]]
    )

    # Apply inverse normalization
    inv_tensor = inv_normalize(image_tensor.clone())

    # Clamp to [0,1] range just in case
    inv_tensor = inv_tensor.clamp(0, 1)

    # Convert to numpy for matplotlib
    np_img = inv_tensor.permute(1, 2, 0).cpu().numpy()
    return np_img



def visualize_ground_truth(dataset, idx):
    image, label, bbox = dataset[idx]
    image = visualize_tensor(image)
    # Undo the normalization if necessary (this example assumes no normalization for simplicity)
    plt.figure(figsize=(6,6))
    plt.imshow(transforms.ToPILImage()(image))
    ax = plt.gca()
    x, y, w, h = bbox.numpy()
    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)
    plt.title("Ground Truth")
    plt.show()

def visualize_image_with_annotation(image, gt_bbox, pred_bbox=None, pred_class=None):
    """
    Displays an image with the ground truth bounding box.
    If provided, also overlays the predicted bounding box and class.
    """
    fig, ax = plt.subplots(1)
    ax.imshow(image)

    # Ground truth bbox in red
    gt_x, gt_y, gt_w, gt_h = gt_bbox
    gt_rect = patches.Rectangle((gt_x, gt_y), gt_w, gt_h, linewidth=2, edgecolor='r', facecolor='none', label='Ground Truth')
    ax.add_patch(gt_rect)

    if pred_bbox is not None:
        pred_x, pred_y, pred_w, pred_h = pred_bbox
        pred_rect = patches.Rectangle((pred_x, pred_y), pred_w, pred_h, linewidth=2, edgecolor='b', facecolor='none', label='Prediction')
        ax.add_patch(pred_rect)
        label_text = "penguin" if pred_class == 1 else "background"
        ax.set_title(f"Predicted: {label_text}")

    plt.legend()
    plt.show()

def debug_on_sample(model, dataset, transform, sample_index=0):
    """
    Visualizes a sample image with both its ground truth and predicted bounding boxes.
    """
    image, label, gt_bbox = dataset[sample_index]

    # Convert tensor image back to PIL image for visualization
    # (Assuming the image was normalized; you may need to reverse the normalization.)
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    image_denorm = inv_normalize(image).clamp(0, 1)
    image_pil = transforms.ToPILImage()(image_denorm)

    # Run inference on the same image
    image_tensor = transform(image_pil).unsqueeze(0)
    with torch.no_grad():
        cls_logits, bbox_pred = model(image_tensor)
    pred_bbox = bbox_pred[0].cpu().numpy()
    pred_class = torch.argmax(cls_logits, dim=1)[0].item()

    visualize_image_with_annotation(image_pil, gt_bbox.numpy(), pred_bbox, pred_class)


def print_sample_predictions(model, dataset, transform, sample_indices=[0, 1, 2, 3, 4]):
    """
    For each sample index in sample_indices, prints the ground truth and predicted bounding boxes and class,
    then visualizes the image with both the ground truth (red) and predicted (blue) bounding boxes.
    """
    model.eval()
    for idx in sample_indices:
        # Retrieve the sample (already transformed and resized) from the dataset.
        image_tensor, label, gt_bbox = dataset[idx]
        print(f"\nSample {idx}:")
        print("Ground Truth bbox:", gt_bbox)

        # Convert tensor image back to PIL image for visualization.
        inv_normalize = transforms.Normalize(
            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
            std=[1/0.229, 1/0.224, 1/0.225]
        )
        image_denorm = inv_normalize(image_tensor).clamp(0, 1)
        image_pil = transforms.ToPILImage()(image_denorm)

        # Run inference on the image.
        image_input = transform(image_pil).unsqueeze(0)  # add batch dimension
        with torch.no_grad():
            cls_logits, bbox_pred = model(image_input)
        pred_bbox = bbox_pred[0].cpu().numpy()
        pred_class = torch.argmax(cls_logits, dim=1)[0].item()

        print("Predicted bbox:", pred_bbox)
        print("Predicted class:", pred_class)

        # Visualize the results.
        visualize_image_with_annotation(image_pil, gt_bbox.numpy(), pred_bbox, pred_class)

import torch
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision import transforms

def debug_bbox(dataset, index):
    """
    Fetch the transformed image & scaled bbox from dataset[index],
    undo the normalization, and plot the bounding box.
    """
    image_tensor, label, bbox = dataset[index]

    # Inverse normalization: (assuming your mean=[0.485, 0.456, 0.406],
    # std=[0.229, 0.224, 0.225]). Adjust if yours differ.
    inv_norm = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )

    # Undo normalization & clamp
    image_denorm = inv_norm(image_tensor).clamp(0, 1)
    # Convert tensor -> HWC -> NumPy
    image_np = image_denorm.permute(1, 2, 0).cpu().numpy()

    # Plot
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.imshow(image_np)

    x, y, w, h = bbox.numpy()
    rect = patches.Rectangle(
        (x, y), w, h,
        linewidth=2, edgecolor='r', facecolor='none'
    )
    ax.add_patch(rect)
    ax.set_title(f"Index {index}, label={label.item()}")
    plt.show()

#@title classes

valid_classes = {"bird",
    "Acadian Flycatcher", "American Crow", "American Goldfinch", "American Pipit",
    "American Redstart", "American Three Toed Woodpecker", "Anna Hummingbird",
    "Artic Tern", "Baird Sparrow", "Baltimore Oriole", "Bank Swallow", "Barn Swallow",
    "Bay Breasted Warbler", "Belted Kingfisher", "Bewick Wren", "Black And White Warbler",
    "Black Billed Cuckoo", "Black Capped Vireo", "Black Footed Albatross", "Black Tern",
    "Black Throated Blue Warbler", "Black Throated Sparrow", "Blue Grosbeak", "Blue Headed Vireo",
    "Blue Jay", "Blue Winged Warbler", "Boat Tailed Grackle", "Bobolink", "Bohemian Waxwing",
    "Brandt Cormorant", "Brewer Blackbird", "Brewer Sparrow", "Bronzed Cowbird", "Brown Creeper",
    "Brown Pelican", "Brown Thrasher", "Cactus Wren", "California Gull", "Canada Warbler",
    "Cape Glossy Starling", "Cape May Warbler", "Cardinal", "Carolina Wren", "Caspian Tern",
    "Cedar Waxwing", "Cerulean Warbler", "Chestnut Sided Warbler", "Chipping Sparrow",
    "Chuck Will Widow", "Clark Nutcracker", "Clay Colored Sparrow", "Cliff Swallow",
    "Common Raven", "Common Tern", "Common Yellowthroat", "Crested Auklet", "Dark Eyed Junco",
    "Downy Woodpecker", "Eared Grebe", "Eastern Towhee", "Elegant Tern", "European Goldfinch",
    "Evening Grosbeak", "Field Sparrow", "Fish Crow", "Florida Jay", "Forsters Tern",
    "Fox Sparrow", "Frigatebird", "Gadwall", "Geococcyx", "Glaucous Winged Gull",
    "Golden Winged Warbler", "Grasshopper Sparrow", "Gray Catbird", "Gray Crowned Rosy Finch",
    "Gray Kingbird", "Great Crested Flycatcher", "Great Grey Shrike", "Green Jay",
    "Green Kingfisher", "Green Tailed Towhee", "Green Violetear", "Groove Billed Ani",
    "Harris Sparrow", "Heermann Gull", "Henslow Sparrow", "Herring Gull", "Hooded Merganser",
    "Hooded Oriole", "Hooded Warbler", "Horned Grebe", "Horned Lark", "Horned Puffin",
    "House Sparrow", "House Wren", "Indigo Bunting", "Ivory Gull", "Kentucky Warbler",
    "Laysan Albatross", "Lazuli Bunting", "Le Conte Sparrow", "Least Auklet", "Least Flycatcher",
    "Least Tern", "Lincoln Sparrow", "Loggerhead Shrike", "Long Tailed Jaeger",
    "Louisiana Waterthrush", "Magnolia Warbler", "Mallard", "Mangrove Cuckoo", "Marsh Wren",
    "Mockingbird", "Mourning Warbler", "Myrtle Warbler", "Nashville Warbler",
    "Nelson Sharp Tailed Sparrow", "Nighthawk", "Northern Flicker", "Northern Fulmar",
    "Northern Waterthrush", "Olive Sided Flycatcher", "Orange Crowned Warbler", "Orchard Oriole",
    "Ovenbird", "Pacific Loon", "Painted Bunting", "Palm Warbler", "Parakeet Auklet",
    "Pelagic Cormorant", "Philadelphia Vireo", "Pied Billed Grebe", "Pied Kingfisher",
    "Pigeon Guillemot", "Pileated Woodpecker", "Pine Grosbeak", "Pine Warbler",
    "Pomarine Jaeger", "Prairie Warbler", "Prothonotary Warbler", "Purple Finch",
    "Red Bellied Woodpecker", "Red Breasted Merganser", "Red Cockaded Woodpecker",
    "Red Eyed Vireo", "Red Faced Cormorant", "Red Headed Woodpecker", "Red Legged Kittiwake",
    "Red Winged Blackbird", "Rhinoceros Auklet", "Ring Billed Gull", "Ringed Kingfisher",
    "Rock Wren", "Rose Breasted Grosbeak", "Ruby Throated Hummingbird", "Rufous Hummingbird",
    "Rusty Blackbird", "Sage Thrasher", "Savannah Sparrow", "Sayornis", "Scarlet Tanager",
    "Scissor Tailed Flycatcher", "Scott Oriole", "Seaside Sparrow", "Shiny Cowbird",
    "Slaty Backed Gull", "Song Sparrow", "Sooty Albatross", "Spotted Catbird",
    "Summer Tanager", "Swainson Warbler", "Tennessee Warbler", "Tree Sparrow", "Tree Swallow",
    "Tropical Kingbird", "Vermilion Flycatcher", "Vesper Sparrow", "Warbling Vireo",
    "Western Grebe", "Western Gull", "Western Meadowlark", "Western Wood Pewee",
    "Whip Poor Will", "White Breasted Kingfisher", "White Breasted Nuthatch",
    "White Crowned Sparrow", "White Eyed Vireo", "White Necked Raven", "White Pelican",
    "White Throated Sparrow", "Wilson Warbler", "Winter Wren", "Worm Eating Warbler",
    "Yellow Bellied Flycatcher", "Yellow Billed Cuckoo", "Yellow Breasted Chat",
    "Yellow Headed Blackbird", "Yellow Throated Vireo", "Yellow Warbler"
}

valid_classes = {name.lower() for name in valid_classes}

penguin_classes = {"Penguin"}

penguin_valid_classes = {name.lower() for name in penguin_classes}

#@title VOC_BirdDataset

import os
import xml.etree.ElementTree as ET
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np

class VOC_BirdDataset(Dataset):
    def __init__(self,
                 root_dir,           # e.g. ".../VOCdevkit/VOC2012"
                 set_filename,       # e.g. "/path/to/bird_train.txt"
                 transform=None,
                 include_background=False):
        """
        :param root_dir: Path to the VOC2012 (or VOC2007) folder that has JPEGImages/, Annotations/, etc.
        :param set_filename: Path to the text file (e.g. bird_train.txt) listing image IDs & presence label (1 or -1).
        :param transform: Albumentations or TorchVision transforms.
        :param include_background: if True, keep images that have no bird as negative samples.
                                   if False, keep only images labeled as bird=1 from the text file.
        """
        self.root_dir = root_dir
        self.image_dir = os.path.join(root_dir, "JPEGImages")
        self.annotation_dir = os.path.join(root_dir, "Annotations")
        self.transform = transform
        self.include_background = include_background

        self.ids = []
        self.labels_map = {}

        background_cnt = 0

        with open(set_filename, 'r') as f:
            for line in f:
                parts = line.strip().split()
                img_id = parts[0]
                label_str = parts[1]
                label_int = int(label_str)  # 1 => has bird, -1 => no bird (or 0 => no bird)
                ann_path = os.path.join(self.annotation_dir, img_id + ".xml")

                if not os.path.exists(ann_path):
                    print(f"Annotation file not found for image {img_id}. Skipping.")
                    continue

                # If we only want images that actually have a bird
                # (include_background=False), skip any labeled -1 or 0
                if not include_background:
                    if label_int == 1:
                        self.ids.append(img_id)
                        self.labels_map[img_id] = label_int
                else:
                  if label_int == 1:
                    # We keep all images: labeled 1 or -1
                    self.ids.append(img_id)
                    self.labels_map[img_id] = label_int
                  elif label_int == -1 and background_cnt < 200:
                    self.ids.append(img_id)
                    self.labels_map[img_id] = label_int
                    background_cnt+= 1

        print(f"Loaded {len(self.ids)} images from {set_filename} (include_background={include_background})")

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        """
        Returns (image_out, boxes, labels, img_id) for a single sample.
        'boxes' is a Tensor of shape [N,4],
        'labels' is a Tensor of shape [N] (1=bird, 0=background).
        If you are storing 'labels_map' from your text file, we incorporate that here.
        """
        img_id = self.ids[idx]

        # Get the "bird vs. no bird" label from the .txt file
        # For example, self.labels_map might say 1 => there's a bird, -1 => no bird
        label_txt = self.labels_map[img_id]  # 1 or -1 (or 0)

        # 1) Load image from JPEGImages/
        img_path = os.path.join(self.image_dir, img_id + ".jpg")
        image = Image.open(img_path).convert("RGB")
        img_width, img_height = image.size

        # 2) Load & parse the XML annotation
        ann_path = os.path.join(self.annotation_dir, img_id + ".xml")

        if not os.path.exists(ann_path):
            # print(f"Annotation file not found for image {img_id}. Skipping.")
            return self.__getitem__((idx + 1) % len(self))
        # else:
        #     print(f"Annotation found to {img_id}. good!!!!.")


        # Try parsing the XML file.
        try:
            tree = ET.parse(ann_path)
            root = tree.getroot()
        except Exception as e:
            print(f"Error parsing annotation for {img_id}: {e}. Skipping.")
            return self.__getitem__((idx + 1) % len(self))

        boxes = []
        labels = []  # we store 1 for bird objects, or maybe 0 for other classes if we do multi-class

        # If label_txt == -1, we treat this as "no bird," i.e. empty bounding boxes
        # unless we want to cross-check the XML to see if there's actually a <object> with name=bird
        if label_txt == 1:
            # According to the text file, there's a bird. Let's parse the bounding box(es).
            # print(f"img_id {img_id}")
            # print(f"img name: {img_id} start")
            for obj in root.findall("object"):
                cls_name = obj.find("name").text.lower().strip()
                # in_classes = cls_name in valid_classes
                # print(f"img name: {img_id} found obj with class name {cls_name} and in? {in_classes}")
                if cls_name in valid_classes:
                    bnd = obj.find("bndbox")
                    xmin = float(bnd.find("xmin").text)
                    ymin = float(bnd.find("ymin").text)
                    xmax = float(bnd.find("xmax").text)
                    ymax = float(bnd.find("ymax").text)

                    # OPTIONAL: clamp box coords so they don't exceed image boundary
                    xmin = max(0, min(xmin, img_width - 1))
                    ymin = max(0, min(ymin, img_height - 1))
                    xmax = max(0, min(xmax, img_width))
                    ymax = max(0, min(ymax, img_height))
                    # print(f"xmin :{xmin}, ymin :{ymin} xmax :{xmax}, ymax :{ymax}")

                    # print(f"img name: {img_id} ,xmin {xmin} ymin{ymin}  xmax{xmax} ymax{ymax} ")
                    if (xmax - xmin) < 1 or (ymax - ymin) < 1:
                        # degenerate box => skip
                        print(f"img name: {img_id} with bbox problem")
                        continue

                    # store [x,y,w,h] or [x1,y1,x2,y2]. We'll keep [x,y,w,h] for example:
                    boxes.append([xmin, ymin, xmax, ymax])  # Change to x1, y1, x2, y2 format
                    labels.append(1)

        # If we have no bounding boxes
        if len(boxes) == 0:
            if self.include_background:
                # Background sample => Explicitly set label 0 and empty bounding box [0,0,0,0]
                boxes = []  # Ensure correct shape
                labels = torch.tensor([0], dtype=torch.long)  # Background label = 0
            else:
                # Skip this image
                print(f"Skipped image {img_id} (background sample without include_background=True)")
                return self.__getitem__((idx + 1) % len(self))


        # Convert to Tensors
        boxes = torch.as_tensor(boxes, dtype=torch.float32)  # shape [N,4]
        labels = torch.as_tensor(labels, dtype=torch.long)    # shape [N]

        # 3) Albumentations or TorchVision transform
        if self.transform:
            import numpy as np
            image_np = np.array(image)  # convert PIL to np

            # Albumentations typically expects boxes in the format you specify:
            # 'coco' => [x_min,y_min,width,height], 'pascal_voc' => [x_min,y_min,x_max,y_max].
            # If you're using [x,y,w,h], that's 'coco' style.
            transformed = self.transform(
                image=image_np,
                bboxes=boxes.tolist(),
                class_labels=labels.tolist()
            )
            image_out = transformed["image"]          # tensor [3,H,W]
            out_boxes = transformed["bboxes"]         # list of [x,y,w,h]
            out_labels = transformed["class_labels"]  # list of ints
            boxes  = torch.as_tensor(out_boxes, dtype=torch.float32)
            labels = torch.as_tensor(out_labels, dtype=torch.long)
        else:
            from torchvision import transforms as T
            to_tensor = T.ToTensor()
            image_out = to_tensor(image)

        return image_out, boxes, labels, img_id

#@title VOC_PenguinDB


import os
import xml.etree.ElementTree as ET
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np

class VOC_PenguinDB(Dataset):
    def __init__(self, root_dir, split="train", transform=None, include_background=False):
        """
        :param root_dir: Path to the dataset directory containing "train/", "valid/", and "test/".
        :param split: One of {"train", "valid", "test"} to specify which subset to use.
        :param transform: Albumentations or TorchVision transforms.
        :param include_background: If True, keep images without a valid object as background samples.
        """
        assert split in ["train", "valid", "test"]

        self.image_dir = os.path.join(root_dir, split)
        self.annotation_dir = os.path.join(root_dir, split)  # Annotations are in the same folder

        self.transform = transform
        self.include_background = include_background

        # ✅ Collect all image filenames (without extensions)
        self.ids = sorted([
            f[:-4] for f in os.listdir(self.image_dir)
            if f.endswith(".jpg") and os.path.exists(os.path.join(self.annotation_dir, f.replace(".jpg", ".xml")))
        ])

        print(f"Loaded {len(self.ids)} images from '{split}' split.")

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        """
        Returns (image_out, boxes, labels, img_id) for a single sample.
        'boxes' is a Tensor of shape [N,4],
        'labels' is a Tensor of shape [N] (1=bird, 0=background).
        """
        img_id = self.ids[idx]

        # 1) Load image
        img_path = os.path.join(self.image_dir, img_id + ".jpg")
        image = Image.open(img_path).convert("RGB")
        img_width, img_height = image.size

        # 2) Load & parse the XML annotation
        ann_path = os.path.join(self.annotation_dir, img_id + ".xml")
        if not os.path.exists(ann_path):
            print(f"Annotation file not found for {img_id}, skipping.")
            return self.__getitem__((idx + 1) % len(self))

        try:
            tree = ET.parse(ann_path)
            root = tree.getroot()
        except Exception as e:
            print(f"Error parsing annotation for {img_id}: {e}, skipping.")
            return self.__getitem__((idx + 1) % len(self))

        boxes = []
        labels = []

        # ✅ Parse objects from the XML
        for obj in root.findall("object"):
            cls_name = obj.find("name").text.lower().strip()
            # print(f"cls_name: {cls_name} for img {img_id} and enterd : {cls_name in penguin_valid_classes}")
            if cls_name in penguin_valid_classes:
                bnd = obj.find("bndbox")
                xmin = float(bnd.find("xmin").text)
                ymin = float(bnd.find("ymin").text)
                xmax = float(bnd.find("xmax").text)
                ymax = float(bnd.find("ymax").text)

                # ✅ Ensure bounding box coordinates are within image bounds
                xmin = max(0, min(xmin, img_width - 1))
                ymin = max(0, min(ymin, img_height - 1))
                xmax = max(0, min(xmax, img_width))
                ymax = max(0, min(ymax, img_height))

                if (xmax - xmin) < 1 or (ymax - ymin) < 1:
                    print(f"Skipping invalid bbox for {img_id}")
                    continue

                boxes.append([xmin, ymin, xmax, ymax])
                labels.append(1)

        # ✅ Handle background samples
        if len(boxes) == 0:
            if self.include_background:
                labels = torch.tensor([0], dtype=torch.long)  # Background label = 0
            else:
                print(f"Skipping background image {img_id} (no valid objects).")
                return self.__getitem__((idx + 1) % len(self))

        # ✅ Convert to Tensors
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.long)

        # 3) Apply Albumentations or TorchVision transforms
        if self.transform:
            image_np = np.array(image)  # Convert PIL to numpy
            transformed = self.transform(
                image=image_np,
                bboxes=boxes.tolist(),
                class_labels=labels.tolist()
            )
            image_out = transformed["image"]  # tensor [3,H,W]
            out_boxes = transformed["bboxes"]
            out_labels = transformed["class_labels"]
            boxes = torch.as_tensor(out_boxes, dtype=torch.float32)
            labels = torch.as_tensor(out_labels, dtype=torch.long)
        else:
            from torchvision import transforms as T
            image_out = T.ToTensor()(image)

        return image_out, boxes, labels, img_id

#@title MultiBirdModel

# --- Model definition (same as before) ---
class MultiBirdModel_Anchors(nn.Module):
    def __init__(self, num_anchors=3, num_classes=1):
        super().__init__()
        backbone = models.resnet18(pretrained=True)
        self.backbone = nn.Sequential(*list(backbone.children())[:-2])

        for name, child in self.backbone.named_children():
            # Freeze only layer1, layer2
            if name in ['layer1','layer2']:
                for param in child.parameters():
                    param.requires_grad = False
            else:
                # layer3, layer4, conv1, bn1 remain trainable
                for param in child.parameters():
                    param.requires_grad = True


        # Now we have feature maps of shape [batch, 512, 7, 7] for input 224x224
        self.conv_head = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))  # or keep a 3x3 or so
        )
        self.conv_cls = nn.Conv2d(512, num_anchors, kernel_size=3, padding=1)   # shape [B,3,7,7] for objectness
        self.conv_box = nn.Conv2d(512, num_anchors*4, kernel_size=3, padding=1) # shape [B,12,7,7] for box offsets


    def forward(self, x):
        feats = self.backbone(x)     # [B,512,7,7]
        # no pooling: we keep [B,512,7,7]
        cls_out = self.conv_cls(feats)   # [B,3,7,7]
        box_out = self.conv_box(feats)   # [B,12,7,7]
        return cls_out, box_out

#@title losses and train functions old!!

# import torch.nn.functional as F

# def focal_loss_bce(logits, targets, alpha=1.0, gamma=2.0):
#     """
#     Basic focal loss for binary classification
#     logits: [B,1,H,W]
#     targets: [B,1,H,W]
#     """
#     bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
#     prob = torch.sigmoid(logits)
#     # focal modulate
#     focal = alpha * (1 - prob)**gamma * bce
#     return focal.mean()



# def voc_collate_fn(batch):
#     images = []
#     all_boxes = []
#     all_labels = []
#     ids = []

#     for sample in batch:
#         img, boxes, labels, img_id = sample
#         images.append(img)

#         # Ensure background samples have at least one bounding box of [0,0,0,0]
#         if len(boxes) == 0:
#             boxes = torch.zeros((1, 4), dtype=torch.float32)  # Fake box for background
#             labels = torch.tensor([0], dtype=torch.long)  # Label 0 for background

#         all_boxes.append(boxes)
#         all_labels.append(labels)
#         ids.append(img_id)

#     # Stack images into a single tensor
#     images = torch.stack(images, dim=0)
#     return images, all_boxes, all_labels, ids


# # --- Loss & metrics (same as before) ---
# def compute_iou_old(box1, box2):
#     x1, y1, w1, h1 = box1
#     x2, y2, w2, h2 = box2
#     x1_br, y1_br = x1 + w1, y1 + h1
#     x2_br, y2_br = x2 + w2, y2 + h2
#     inter_x1 = max(x1, x2)
#     inter_y1 = max(y1, y2)
#     inter_x2 = min(x1_br, x2_br)
#     inter_y2 = min(y1_br, y2_br)
#     inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
#     union_area = w1 * h1 + w2 * h2 - inter_area
#     return inter_area / union_area if union_area > 0 else 0

# def calculate_map(pred_boxes, gt_boxes, iou_threshold=0.5):
#     ious = [compute_iou(p, g) for p, g in zip(pred_boxes, gt_boxes)]
#     return sum(ious) / len(ious) if ious else 0

# # --- Training and validation functions ---
# def train_one_epoch(model, loader, optimizer, device):
#     model.train()
#     total_loss = 0.0

#     # define your losses outside the loop:
#     cls_loss_fn = nn.BCEWithLogitsLoss(reduction='none')
#     box_loss_fn = nn.SmoothL1Loss(reduction='none')

#     for images, list_of_boxes, list_of_labels, ids in loader:
#         images = images.to(device)
#         cls_map_out, box_map_out = model(images)  # shape [B,1,7,7], [B,4,7,7]

#         # 1) Build the target Tensors
#         cls_target, box_target, obj_mask = build_targets(cls_map_out, box_map_out, list_of_boxes, list_of_labels)


#         # 2) Classification loss
#         # We only compute classification loss = 1 vs 0. We can do:
#         cls_loss_all = focal_loss_bce(cls_map_out, cls_target)  # shape [B,1,7,7]
#         # You might want to weigh positives more than negatives, or do a focal loss, etc.
#         cls_loss = cls_loss_all.mean()

#         box_loss_all = box_loss_fn(box_map_out, box_target)   # both [B,12,7,7]

#         # 3) Box regression loss
#         # We only compute it where obj_mask==1
#         obj_mask_4 = obj_mask.repeat_interleave(4, dim=1)  # e.g. [B,3->12,7,7]
#         box_loss_all = box_loss_all * obj_mask_4
#         box_loss = box_loss_all.sum() / (obj_mask_4.sum() + 1e-6)

#         # total loss
#         # loss = cls_loss + 3.0 * box_loss  # might scale box loss if needed
#         loss = cls_loss + 10 * box_loss  # Increase importance of bbox regression
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()

#         total_loss += loss.item()

#     return total_loss / len(loader)


# from torchvision.ops import nms

# import torch
# from torchvision.ops import nms

# def decode_predictions(cls_map_out, box_map_out, score_thresh=0.3, iou_thresh=0.5):
#     """
#     cls_map_out: [B, num_anchors, 7, 7]
#     box_map_out: [B, num_anchors*4, 7, 7]
#     score_thresh: only keep boxes with sig(sigmoid) > this
#     iou_thresh: used for NMS
#     Returns:
#       a list of length B, each element is (final_boxes, final_scores)
#         final_boxes: shape [N,4] in [x1,y1,x2,y2]
#         final_scores: shape [N]
#     """
#     from torchvision.ops import nms

#     B, num_anchors, H, W = cls_map_out.shape
#     all_outputs = []

#     for b in range(B):
#         box_list = []
#         score_list = []

#         for anchor_idx in range(num_anchors):
#             # indices in box_map_out for this anchor
#             ch_start = anchor_idx*4
#             for i in range(H):
#                 for j in range(W):
#                     # classification (objectness)
#                     score_logit = cls_map_out[b, anchor_idx, i, j]
#                     score = torch.sigmoid(score_logit)
#                     if score < score_thresh:
#                         continue  # skip low-conf detections

#                     # read predicted x1,y1,w,h from the box_map
#                     x1 = box_map_out[b, ch_start + 0, i, j]
#                     y1 = box_map_out[b, ch_start + 1, i, j]
#                     w  = box_map_out[b, ch_start + 2, i, j]
#                     h  = box_map_out[b, ch_start + 3, i, j]

#                     # convert (x1,y1,w,h) => x2,y2
#                     x2 = x1 + w
#                     y2 = y1 + h

#                     # collect them
#                     box_list.append([x1.item(), y1.item(), x2.item(), y2.item()])
#                     score_list.append(score.item())

#         if len(box_list) == 0:
#             # No detections at all => return empty
#             final_boxes = torch.zeros((0,4), dtype=torch.float32)
#             final_scores = torch.zeros((0,), dtype=torch.float32)
#             all_outputs.append((final_boxes, final_scores))
#             continue

#         # Convert to Torch tensors
#         boxes_tensor = torch.tensor(box_list, dtype=torch.float32)
#         scores_tensor = torch.tensor(score_list, dtype=torch.float32)

#         # run NMS
#         keep_indices = nms(boxes_tensor, scores_tensor, iou_thresh)
#         final_boxes  = boxes_tensor[keep_indices]
#         final_scores = scores_tensor[keep_indices]

#         all_outputs.append((final_boxes, final_scores))

#     return all_outputs



# import torch

# def compute_iou(box_a, box_b):
#     """
#     box_a, box_b: [x1,y1,x2,y2] (Tensor)
#     Returns scalar IoU
#     """
#     x1 = max(box_a[0], box_b[0])
#     y1 = max(box_a[1], box_b[1])
#     x2 = min(box_a[2], box_b[2])
#     y2 = min(box_a[3], box_b[3])
#     inter_w = max(0, x2 - x1)
#     inter_h = max(0, y2 - y1)
#     inter_area = inter_w * inter_h

#     area_a = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])
#     area_b = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])
#     union_area = area_a + area_b - inter_area

#     iou = inter_area / union_area if union_area > 0 else 0
#     return iou

# def compute_ap50(pred_boxes, gt_boxes, iou_threshold=0.5):
#     """
#     pred_boxes, gt_boxes: lists of [x1,y1,x2,y2]
#     This is a simplified single-image AP@IoU>=0.5
#     """
#     if len(pred_boxes) == 0:
#         return 1.0 if len(gt_boxes) == 0 else 0.0

#     # Mark each prediction as TP or FP, then compute precision
#     # Sort predictions by confidence if you have them (omitted here for brevity)
#     tp = 0
#     fp = 0
#     matched_gt = set()

#     for i, pb in enumerate(pred_boxes):
#         best_iou = 0
#         best_gt_idx = -1
#         for j, gb in enumerate(gt_boxes):
#             iou = compute_iou(pb, gb)
#             if iou > best_iou:
#                 best_iou = iou
#                 best_gt_idx = j
#         if best_iou >= iou_threshold and best_gt_idx not in matched_gt:
#             tp += 1
#             matched_gt.add(best_gt_idx)
#         else:
#             fp += 1

#     fn = len(gt_boxes) - len(matched_gt)
#     # Basic precision, recall:
#     precision = tp / (tp + fp) if (tp + fp) > 0 else 0
#     recall = tp / (tp + fn) if (tp + fn) > 0 else 0

#     # Here, we'll just return precision for a rough measure,
#     # or you can do a typical AP calculation across multiple recall points.
#     return precision

# # Usage example inside validation loop:
# # - pred_boxes: predicted bboxes [N,4]
# # - gt_boxes: groundtruth bboxes [M,4]
# # - This function is a stub; for real mAP you would gather predictions over
# #   an entire dataset, sort by confidence, sample recall points, etc.



# def compute_average_iou(pred_boxes, gt_boxes):
#     """
#     Simple function to compute an average IoU between predicted boxes
#     and ground-truth boxes. A naive approach might do best-match or
#     just compute an average of pairwise max IoU.
#     This is just an example placeholder.
#     """
#     if len(pred_boxes) == 0 or len(gt_boxes) == 0:
#         return 0.0

#     # a typical approach: for each gt box, find the max IoU with any predicted box
#     ious = []
#     for gt in gt_boxes:
#         iou_max = 0.0
#         for pred in pred_boxes:
#             iou_val = single_iou(gt, pred)
#             if iou_val > iou_max:
#                 iou_max = iou_val
#         ious.append(iou_max)

#     # average over ground-truth boxes
#     return sum(ious) / len(ious)

# def single_iou(boxA, boxB):
#     """Compute IoU of two boxes in x1,y1,x2,y2."""
#     xA = max(boxA[0], boxB[0])
#     yA = max(boxA[1], boxB[1])
#     xB = min(boxA[2], boxB[2])
#     yB = min(boxA[3], boxB[3])

#     interArea = max(0, xB - xA) * max(0, yB - yA)
#     boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
#     boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
#     iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)
#     return iou

# def validate(model, dataloader, device):
#     """
#     Runs validation loop, decoding the model outputs
#     into bounding boxes, comparing with ground truth.
#     Returns a single 'val_score' for example
#     (an average IoU or something similar).
#     """
#     model.eval()
#     total_iou = 0.0
#     count = 0

#     for images, list_of_boxes, list_of_labels, ids in dataloader:
#         images = images.to(device)

#         with torch.no_grad():
#             cls_map_out, box_map_out = model(images)

#         # decode predicted boxes
#         batch_preds = decode_predictions(cls_map_out, box_map_out,
#                                          score_thresh=0.3, iou_thresh=0.6)

#         # For each image, measure IoU
#         # Our list_of_boxes is presumably shape [N,4] in x,y,w,h or x1,y1,x2,y2
#         # If they're x,y,w,h, convert them to x1,y1,x2,y2 first
#         # Then compute an average iou
#         for i in range(len(images)):
#             gt_raw = list_of_boxes[i]  # shape [N,4]
#             # Convert [x,y,w,h] -> [x1,y1,x2,y2] if needed
#             # If your code already uses that format, skip:
#             gt_boxes_xyxy = []
#             for box in gt_raw:
#                 # x1 = box[0].item()
#                 # y1 = box[1].item()
#                 # x2 = x1 + box[2].item()
#                 # y2 = y1 + box[3].item()
#                 x1 = box[0].item()
#                 y1 = box[1].item()
#                 x2 = box[2].item()
#                 y2 = box[3].item()
#                 gt_boxes_xyxy.append([x1, y1, x2, y2])
#             gt_boxes_xyxy = torch.tensor(gt_boxes_xyxy, dtype=torch.float32)

#             pred_boxes_xyxy, pred_scores = batch_preds[i]  # shape [M,4], [M]
#             ap50_value = compute_ap50(pred_boxes_xyxy.tolist(), gt_boxes_xyxy.tolist(), iou_threshold=0.5)
#             print("AP50:", ap50_value)
#             # compute an average iou
#             iou_val = compute_average_iou(pred_boxes_xyxy, gt_boxes_xyxy)
#             total_iou += iou_val
#             count += 1

#     val_score = total_iou / max(count, 1)
#     return val_score



# def train_model(device,train_loader,val_loader):
#     writer = SummaryWriter(log_dir="/content/drive/MyDrive/runs/birds_detection")

#     model = MultiBirdModel_Anchors(num_classes=2).to(device)
#     optimizer = optim.Adam(model.parameters(), lr=1e-5)

#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

#     EPOCHS = 10

#     for epoch in range(EPOCHS):
#         print(f"\n=== Epoch {epoch+1}/{EPOCHS} ===")

#         # train_one_epoch will do the build_targets, compute losses, etc.
#         train_loss = train_one_epoch(model, train_loader, optimizer, device)

#         # Optionally do validation
#         # For multi-object detection, you'd decode predictions in validate(...)
#         val_score = validate(model, val_loader, device)

#         print(f"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Score: {val_score:.4f}")
#         scheduler.step()
#     # Save the model.
#     torch.save(model.state_dict(), model_path)
#     print("Training complete and model saved.")
#     writer.close()

#@title losses and train functions

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.ops import nms

def collate_fn(batch):
    """
    Custom collate function to handle variable-sized bounding boxes.
    Returns a batch where images are stacked, but boxes & labels are lists of different lengths.
    """
    images = []
    boxes = []
    labels = []
    img_ids = []

    for img, box, label, img_id in batch:
        images.append(img)
        boxes.append(box)  # Keep as list (do not stack)
        labels.append(label)  # Keep as list (do not stack)
        img_ids.append(img_id)

    images = torch.stack(images, 0)  # Stack only images

    return images, boxes, labels, img_ids


##############################################################################
# 1) build_targets
##############################################################################
def build_targets(cls_map, box_map, list_of_boxes, list_of_labels):
    """
    cls_map:  [B, num_anchors, 7, 7]   (raw network output shape)
    box_map:  [B, num_anchors*4, 7, 7] (raw network output shape)
    list_of_boxes: length B, each is a Tensor [N,4] in [x1,y1,x2,y2], scaled to 224x224
    list_of_labels: length B, each is a Tensor [N] (1=bird), ignoring background
    Returns:
      cls_target: same shape as cls_map
      box_target: same shape as box_map
      obj_mask:   same shape as cls_map  (1 => object present at anchor/cell)
    """
    B, num_anchors, H, W = cls_map.shape  # e.g. [B,3,7,7]
    # box_map: [B, num_anchors*4, H, W]

    # Make empty targets
    cls_target = torch.zeros_like(cls_map)      # [B,3,7,7]
    box_target = torch.zeros_like(box_map)      # [B,12,7,7] if num_anchors=3
    obj_mask   = torch.zeros_like(cls_map)      # [B,3,7,7]

    # Example anchor sizes (in "pixel" space for 224×224 images)
    anchor_sizes = [(32,32), (64,64), (96,96)]
    # If you only want 1 anchor, set anchor_sizes = [(64,64)] and num_anchors=1.

    # Each cell is ~32 pixels wide/tall if 7×7 + 224×224
    cell_w = 224.0 / W
    cell_h = 224.0 / H

    for b in range(B):
        boxes_b  = list_of_boxes[b]  # shape [N,4], each row [x1,y1,x2,y2]
        labels_b = list_of_labels[b] # shape [N], typically all 1 for "bird"

        for n in range(len(boxes_b)):
            x1 = boxes_b[n,0].item()
            y1 = boxes_b[n,1].item()
            x2 = boxes_b[n,2].item()
            y2 = boxes_b[n,3].item()

            w  = x2 - x1
            h  = y2 - y1
            cx = x1 + w/2
            cy = y1 + h/2

            # Identify which cell in the 7×7 grid
            cell_x = int(cx // cell_w)
            cell_y = int(cy // cell_h)
            cell_x = max(0, min(cell_x, W-1))
            cell_y = max(0, min(cell_y, H-1))

            # Quick approach: pick anchor=0 only
            # For real multi-anchor detection, pick whichever anchor has best IoU with (w,h).
            best_anchor = -1
            best_iou = 0

            for anchor_idx, (aw, ah) in enumerate(anchor_sizes):
                iou = compute_iou([0, 0, w, h], [0, 0, aw, ah])  # IoU עם כל Anchor Box
                if iou > best_iou:
                    best_iou = iou
                    best_anchor = anchor_idx



            # Mark objectness
            obj_mask[b, best_anchor, cell_y, cell_x] = 1.0
            cls_target[b, best_anchor, cell_y, cell_x] = 1.0  # single class => 1

            # box_target has 4 channels per anchor => anchor0 in [0..3], anchor1 in [4..7], etc.
            box_idx = best_anchor * 4
            box_target[b, box_idx+0, cell_y, cell_x] = x1
            box_target[b, box_idx+1, cell_y, cell_x] = y1
            box_target[b, box_idx+2, cell_y, cell_x] = w
            box_target[b, box_idx+3, cell_y, cell_x] = h

    return cls_target, box_target, obj_mask


##############################################################################
# 2) decode_predictions
##############################################################################
def decode_predictions(cls_map_out, box_map_out, score_thresh=0.3, iou_thresh=0.5):
    """
    cls_map_out: [B, num_anchors, 7, 7]
    box_map_out: [B, num_anchors*4, 7, 7]
    - We assume each anchor's 4 channels represent: (x1, y1, w, h).
    - We convert that to (x1,y1,x2,y2) and apply NMS.
    Returns: list of length B => (final_boxes, final_scores),
      final_boxes: [N,4] in [x1,y1,x2,y2]
      final_scores: [N]
    """
    B, num_anchors, H, W = cls_map_out.shape
    batch_out = []

    for b in range(B):
        box_candidates = []
        score_candidates = []

        for anchor_idx in range(num_anchors):
            ch_start = anchor_idx * 4

            for i in range(H):
                for j in range(W):
                    score_logit = cls_map_out[b, anchor_idx, i, j]
                    score = torch.sigmoid(score_logit)
                    if score < score_thresh:
                        continue

                    # box predicted in [x1,y1,w,h]
                    x1 = box_map_out[b, ch_start+0, i, j]
                    y1 = box_map_out[b, ch_start+1, i, j]
                    w  = box_map_out[b, ch_start+2, i, j]
                    h  = box_map_out[b, ch_start+3, i, j]

                    x2 = x1 + w
                    y2 = y1 + h

                    box_candidates.append([x1.item(), y1.item(), x2.item(), y2.item()])
                    score_candidates.append(score.item())

        if len(box_candidates) == 0:
            # No boxes => empty
            final_boxes  = torch.zeros((0,4), dtype=torch.float32)
            final_scores = torch.zeros((0,),  dtype=torch.float32)
            batch_out.append((final_boxes, final_scores))
            continue

        # Convert to Tensors
        boxes_tensor  = torch.tensor(box_candidates,  dtype=torch.float32)
        scores_tensor = torch.tensor(score_candidates, dtype=torch.float32)

        # Non-Max Suppression
        keep_indices = nms(boxes_tensor, scores_tensor, iou_thresh)
        final_boxes  = boxes_tensor[keep_indices]
        final_scores = scores_tensor[keep_indices]

        batch_out.append((final_boxes, final_scores))

    return batch_out


##############################################################################
# 3) train_one_epoch
##############################################################################
def train_one_epoch(model, loader, optimizer, device):
    """
    1) Extract a batch of images + ground-truth boxes
    2) 'build_targets()' => produce classification & regression targets
    3) Forward pass => compute classification (BCE) and box (SmoothL1) losses
    4) Backprop + update
    """
    model.train()
    total_loss = 0.0

    # We'll define a simple BCE for classification, SmoothL1 for box regression
    cls_loss_fn = nn.BCEWithLogitsLoss(reduction='none')
    box_loss_fn = nn.SmoothL1Loss(reduction='none')

    for images, list_of_boxes, list_of_labels, _ in loader:
        images = images.to(device)

        # Forward pass
        cls_map_out, box_map_out = model(images)  # e.g. [B,3,7,7], [B,12,7,7]

        # Build target maps
        cls_target, box_target, obj_mask = build_targets(
            cls_map_out, box_map_out, list_of_boxes, list_of_labels
        )
        cls_target = cls_target.to(device)
        box_target = box_target.to(device)
        obj_mask   = obj_mask.to(device)

        # Classification loss
        cls_loss_all = cls_loss_fn(cls_map_out, cls_target)  # shape [B,3,7,7]
        # We can average across all anchors/cells
        cls_loss = cls_loss_all.mean()

        # Box loss
        # We only compute where obj_mask==1
        box_loss_all = box_loss_fn(box_map_out, box_target)  # shape [B,12,7,7]
        obj_mask_4   = obj_mask.repeat_interleave(4, dim=1)  # e.g. [B,3->12,7,7]
        box_loss_all = box_loss_all * obj_mask_4
        # Avoid dividing by zero
        box_loss = box_loss_all.sum() / (obj_mask_4.sum() + 1e-6)

        # Combine
        loss = cls_loss + 5.0 * box_loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(loader)
    return avg_loss


##############################################################################
# 4) validate
##############################################################################
def compute_iou(box_a, box_b):
    """
    box_a, box_b: [x1,y1,x2,y2]
    Returns scalar IoU
    """
    x1 = max(box_a[0], box_b[0])
    y1 = max(box_a[1], box_b[1])
    x2 = min(box_a[2], box_b[2])
    y2 = min(box_a[3], box_b[3])
    inter_w = max(0, x2 - x1)
    inter_h = max(0, y2 - y1)
    inter_area = inter_w * inter_h

    area_a = (box_a[2]-box_a[0])*(box_a[3]-box_a[1])
    area_b = (box_b[2]-box_b[0])*(box_b[3]-box_b[1])
    union = area_a + area_b - inter_area
    return inter_area / union if union>0 else 0

def compute_ap50(pred_boxes, gt_boxes, iou_threshold=0.5):
    """
    pred_boxes, gt_boxes: lists of [x1,y1,x2,y2]
    Return a "precision" measure for AP@0.5 as a simple single-value stub.
    """
    if len(pred_boxes)==0:
        return 1.0 if len(gt_boxes)==0 else 0.0

    matched_gt = set()
    tp, fp = 0, 0
    # (In a true detection pipeline, you'd sort by confidence, do multiple recall thresholds, etc.
    #  This is a minimal placeholder.)
    for pb in pred_boxes:
        best_iou, best_idx = 0.0, -1
        for i, gb in enumerate(gt_boxes):
            iou_val = compute_iou(pb, gb)
            if iou_val>best_iou:
                best_iou = iou_val
                best_idx = i
        if best_iou>=iou_threshold and best_idx not in matched_gt:
            tp += 1
            matched_gt.add(best_idx)
        else:
            fp += 1
    fn = len(gt_boxes)-len(matched_gt)

    precision = tp/(tp+fp) if (tp+fp)>0 else 0
    return precision

def validate(model, loader, device):
    """
    1) Model.eval()
    2) decode_predictions => get [x1,y1,x2,y2] for each image
    3) compare with GT => compute AP50
    Returns an average AP across the entire loader
    """
    model.eval()
    sum_ap, count = 0.0, 0

    for images, list_of_boxes, list_of_labels, _ in loader:
        images = images.to(device)

        with torch.no_grad():
            cls_map_out, box_map_out = model(images)

        # decode predictions
        batch_outputs = decode_predictions(cls_map_out, box_map_out,
                                           score_thresh=0.3,
                                           iou_thresh=0.5)
        # measure AP for each image
        B = len(images)
        for i in range(B):
            # predicted boxes are in batch_outputs[i][0]
            pred_boxes_xyxy, _ = batch_outputs[i]
            # ground-truth boxes are in list_of_boxes[i], shape [N,4] => [x1,y1,x2,y2]
            gt_boxes = list_of_boxes[i].cpu().numpy().tolist()

            # convert pred_boxes_xyxy to list
            pred_boxes_list = pred_boxes_xyxy.cpu().numpy().tolist()

            ap50 = compute_ap50(pred_boxes_list, gt_boxes, iou_threshold=0.5)
            sum_ap += ap50
            count  += 1

    mean_ap = sum_ap/max(count,1)
    return mean_ap


##############################################################################
# 5) train_model
##############################################################################
def train_model(model, train_loader, val_loader, device, epochs=10):
    """
    Example training loop that calls train_one_epoch & validate
    """
    writer = SummaryWriter(log_dir="/content/drive/MyDrive/runs/birds_detection")

    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    # Example: drop LR by 10× at epoch 15, then again at epoch 25
    scheduler = torch.optim.lr_scheduler.MultiStepLR(
        optimizer, milestones=[15, 25], gamma=0.1
    )
    best_ap = 0
    patience = 10
    counter = 0

    for epoch in range(epochs):
        print(f"\n=== Epoch {epoch+1}/{epochs} ===")
        train_loss = train_one_epoch(model, train_loader, optimizer, device)
        val_ap = validate(model, val_loader, device)
        print(f"Epoch {epoch+1}/{epochs} | Loss={train_loss:.4f}, AP50={val_ap:.3f}")
        if val_ap > best_ap:
            best_ap = val_ap
            counter = 0
            torch.save(model.state_dict(), model_path)
            print("✅ Best model so far — saved!")
        else:
            counter += 1
            print(f"⚠️ No improvement. Patience: {counter}/{patience}")
            if counter >= patience:
                print("⏹️ Early stopping — no improvement in validation AP.")
                break
        scheduler.step()  # reduce LR if we hit a milestone

    torch.save(model.state_dict(), model_path)
    print("Training complete!")
    return model

#@title build targets

def build_targets(cls_map, box_map, list_of_boxes, list_of_labels):
    B, num_anchors, H, W = cls_map.shape
    cls_target = torch.zeros_like(cls_map)
    box_target = torch.zeros_like(box_map)
    obj_mask   = torch.zeros_like(cls_map)

    anchor_sizes = [(32,32),(64,64),(96,96)]  # or more anchors if you like
    cell_w = 224.0 / W
    cell_h = 224.0 / H

    for b in range(B):
        boxes_b  = list_of_boxes[b]
        labels_b = list_of_labels[b]

        for n in range(len(boxes_b)):
            x1 = boxes_b[n,0].item()
            y1 = boxes_b[n,1].item()
            x2 = boxes_b[n,2].item()
            y2 = boxes_b[n,3].item()

            w  = x2 - x1
            h  = y2 - y1
            cx = x1 + w/2
            cy = y1 + h/2
            cell_x = int(cx//cell_w)
            cell_y = int(cy//cell_h)
            cell_x = max(0,min(cell_x,W-1))
            cell_y = max(0,min(cell_y,H-1))

            # compute IoU with each anchor
            best_iou = 0
            best_anchor = 0
            for a_idx, (aw,ah) in enumerate(anchor_sizes):
                # approximate iou in w,h space only
                inter_w = min(w, aw)
                inter_h = min(h, ah)
                inter_area = max(0, inter_w*inter_h)
                union_area = w*h + aw*ah - inter_area
                iou = inter_area/union_area if union_area>0 else 0
                if iou>best_iou:
                    best_iou = iou
                    best_anchor = a_idx

            obj_mask[b, best_anchor, cell_y, cell_x] = 1.0
            cls_target[b, best_anchor, cell_y, cell_x] = 1.0

            box_idx = best_anchor*4
            box_target[b, box_idx+0, cell_y, cell_x] = x1
            box_target[b, box_idx+1, cell_y, cell_x] = y1
            box_target[b, box_idx+2, cell_y, cell_x] = w
            box_target[b, box_idx+3, cell_y, cell_x] = h

    return cls_target, box_target, obj_mask

#@title video

import cv2
import torch
import numpy as np
from PIL import Image
from torchvision import transforms

def inference_on_video(
    model,
    video_path,
    output_video_path,
    device='coda',
    score_thresh=0.35,
    iou_thresh=0.6
):
    """
    Runs the given multi-object detection model on each frame of a video.
    Decodes the predicted feature maps to multiple bounding boxes,
    draws any with confidence > score_thresh,
    and writes out an annotated video.

    Args:
      model (nn.Module): Trained detection model, in eval mode
      video_path (str): Path to input video file
      output_video_path (str): Path where annotated video will be saved
      device (str): "cpu" or "cuda"
      score_thresh (float): Probability threshold for drawing predicted boxes
      iou_thresh (float): NMS IoU threshold in decode_predictions
    """

    # 1) Prepare transforms: same as your test/val
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485,0.456,0.406],
                             std=[0.229,0.224,0.225])
    ])

    # 2) Setup video capture & writer
    cap = cv2.VideoCapture(video_path)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or "XVID" etc.
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    out = cv2.VideoWriter(
        output_video_path,
        fourcc,
        fps,
        (frame_w, frame_h)
    )

    # Make sure your decode_predictions is imported or defined.
    # For example:
    # from your_script import decode_predictions

    model.eval()
    frame_index = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # 3) Convert current frame to the format your model expects
        #    (PIL -> transform -> Tensor)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        image_tensor = transform(pil_image).unsqueeze(0).to(device)  # shape=[1,3,224,224]

        # 4) Inference to get raw feature map outputs
        with torch.no_grad():
            cls_map_out, box_map_out = model(image_tensor)
            # shape [1,1,7,7], [1,4,7,7]

        # 5) Decode to multiple bounding boxes
        # decode_predictions returns a list of length B=1 => (boxes, scores)
        batch_preds = decode_predictions(cls_map_out, box_map_out,
                                         score_thresh=score_thresh,
                                         iou_thresh=iou_thresh)
        # batch_preds[0] is (boxes, scores) for this single frame
        (boxes_xyxy, scores) = batch_preds[0]

        # 6) Rescale and draw each predicted box
        # boxes_xyxy is shape [N,4] in [x1,y1,x2,y2] at 224×224 scale
        scale_x = frame_w / 224.0
        scale_y = frame_h / 224.0

        for i in range(len(boxes_xyxy)):
            x1, y1, x2, y2 = boxes_xyxy[i]
            conf = scores[i]
            # optionally you can filter here if conf<score_thresh,
            # but decode_predictions is already skipping via raw sigmoid

            # rescale to the actual frame size
            X1 = int(x1 * scale_x)
            Y1 = int(y1 * scale_y)
            X2 = int(x2 * scale_x)
            Y2 = int(y2 * scale_y)

            # draw a bounding box in BGR color (0,0,255)=red
            cv2.rectangle(
                frame,
                (X1, Y1),
                (X2, Y2),
                (0, 0, 255),
                2
            )

            label_text = f"Penguin: {conf:.2f}"
            cv2.putText(
                frame,
                label_text,
                (X1, max(Y1 - 10, 0)),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.8,
                (0, 0, 255),
                2
            )

        # 7) Write annotated frame to output video
        out.write(frame)
        frame_index += 1

    cap.release()
    out.release()
    print(f"Inference complete! Annotated video saved as: {output_video_path}")

#@title main

import albumentations as A
from albumentations.pytorch import ToTensorV2

# --- Main pipeline with TensorBoard logging ---
def main():

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Setup TensorBoard writer.

    # Modify these paths to point to your actual directories and JSON files.
    general_dir = "/content/drive/MyDrive/penguin_db"


    import albumentations as A
    from albumentations.pytorch import ToTensorV2

    train_transform = A.Compose([
        A.Resize(256, 256),
        A.RandomSizedBBoxSafeCrop(224, 224),
        A.RandomCrop(224, 224),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.MotionBlur(p=0.2),
        A.Perspective(p=0.2),
        A.GaussNoise(p=0.3),
        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
        ToTensorV2(),
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))




    val_transform = A.Compose([
                                A.Resize(224,224),
                                A.Normalize(mean=(0.485,0.456,0.406),
                                            std=(0.229,0.224,0.225)),
                                ToTensorV2()
                                  ],
                                      bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])
                                  )


    train_dataset = VOC_PenguinDB(
      root_dir=general_dir,
      split="train",
      transform=train_transform,
      include_background=False
    )

    val_dataset = VOC_PenguinDB(
        root_dir=general_dir,
        split="valid",
        transform=val_transform,
        include_background=False
    )

    from torch.utils.data import Subset

    half_len = len(train_dataset)
    print(f"Half of the dataset length: {half_len}")
    train_dataset = Subset(train_dataset, list(range(int(half_len))))

    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Validation dataset size: {len(val_dataset)}")

    # debug_ground_truth(train_dataset, num_samples=20)

    train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True,collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False,collate_fn=collate_fn)

    # Initialize the model.
    model = MultiBirdModel_Anchors(num_classes=2).to(device)

    train_model(model,train_loader,val_loader,device,epochs=60)

        # Make sure the model architecture matches the one you trained.
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    model.to(device)
    model.eval()  # set to evaluation mode

    inference_on_video(
        model=model,
        video_path="/content/drive/MyDrive/emperor_penguins.mp4",
        output_video_path="/content/drive/MyDrive/penguinpart91000000.mp4",
        device=device,
    )


    # run_visualize_test(
    #     model_path=model_path,
    #     root_dir="/content/drive/MyDrive/voc_data",
    #     set_filename="/content/drive/MyDrive/voc_data/ImageSets/Main/bird_val.txt",
    #     device='cuda'
    # )

if __name__ == "__main__":
    main()

import albumentations as A
from albumentations.pytorch import ToTensorV2
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    general_dir = "/content/drive/MyDrive/penguin_db"
    model_path = "/content/drive/MyDrive/voc_data/155penguin_multy_0319.pth"

    # Transforms


    # Datasets and loaders
    train_transform = A.Compose([
        A.Resize(256, 256),
        A.RandomCrop(224, 224),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.MotionBlur(p=0.2),
        A.Perspective(p=0.2),
        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
        ToTensorV2(),
    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))




    val_transform = A.Compose([
                                A.Resize(224,224),
                                A.Normalize(mean=(0.485,0.456,0.406),
                                            std=(0.229,0.224,0.225)),
                                ToTensorV2()
                                  ],
                                      bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'])
                                  )

    train_dataset = VOC_PenguinDB(
      root_dir=general_dir,
      split="train",
      transform=train_transform,
      include_background=False
    )

    val_dataset = VOC_PenguinDB(
        root_dir=general_dir,
        split="valid",
        transform=val_transform,
        include_background=False
    )


    train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)

    # 🔁 Load pre-trained model (skip training if already trained)
    model = MultiBirdModel_Anchors(num_classes=2).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()  # set to evaluation mode
    print("✅ Loaded pre-trained model")

    # 🔁 (Optional) comment this out if you don't want to train again
    # train_model(model, train_loader, val_loader, device, epochs=20)

    # Inference on external video
    inference_on_video(
        model=model,
        video_path="/content/drive/MyDrive/pen.mp4",
        output_video_path="/content/drive/MyDrive/parttttt13.mp4",
        device=device,
    )

if __name__ == "__main__":
      main()